{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3573f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sentencepiece peft accelerate bitsandbytes comet-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fcef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga2_7b_lora\"\n",
    "# MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}: {content}</s>\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"Ты — Артем Заболотный. Тебе 26 лет, ты закончил бакалавриат \"\n",
    "    \" в университет аэрокосмического приборостроения, магистратуру в Сколтехе и сейчас \"\n",
    "    \" учишься в аспирантуре Сколтеха. Ты работал с 2017 по 2020 в Радарио full-stack разработчиком на\"\n",
    "    \" C#, GO, VUE, а сейчас работаешь Senior Data Scientist в RnD отделе сбербанка\"\n",
    "    \" у тебя есть девушка и ее зовут Марина, ты живешь с ней в Тбилиси\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab26729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"COMET_API_KEY\"] = \"loSavVwkQth0fYeWQk1774LlK\"\n",
    "os.environ[\"COMET_PROJECT_NAME\"] = \"digital-twin-llora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"IlyaGusev/saiga2_7b_lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97459adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8afa81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path, torch_dtype=torch.float16, device_map=\"mps\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, MODEL_NAME, torch_dtype=torch.float16)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c826ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_path = f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\"\n",
    "        checkpoint_folder = os.path.join(args.output_dir, checkpoint_path)\n",
    "        kwargs[\"model\"].save_pretrained(checkpoint_folder)\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb832c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_lora = LoraConfig(\n",
    "    peft_type=\"LORA\",\n",
    "    auto_mapping=None,\n",
    "    base_model_name_or_path=\"MyModel\",\n",
    "    revision=None,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    fan_in_fan_out=False,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=None,\n",
    "    init_lora_weights=True,\n",
    "    layers_to_transform=None,\n",
    "    layers_pattern=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5817808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_lora = get_peft_model(model, dialog_lora, adapter_name=\"dialog_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.Dataset import ChatDataset\n",
    "from src.preprocessing.utils import read_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e04a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = read_jsonl(\"../data/results/tg_conversation.jsonl\")\n",
    "MODEL_NAME = \"IlyaGusev/saiga2_7b_lora\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "records[1][\"messages\"] = records[1][\"messages\"][:7]\n",
    "dataset = ChatDataset(\n",
    "    original_records=records[:3],\n",
    "    templates_path=\"../data/templates/chat_conversation_template.json\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_tokens_count=2048,\n",
    ")\n",
    "labels = dataset[1][\"labels\"]\n",
    "labels[labels == -100] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"LLAMA7B-dialog-lora-v2\",\n",
    "    learning_rate=6e-5,\n",
    "    weight_decay=0.001,\n",
    "    push_to_hub=True,\n",
    "    hub_private_repo=True,\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=128,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=30,  # use 3 or 5 epochs here\n",
    "    report_to=\"comet_ml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b03fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
